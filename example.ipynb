{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchkit.pytorch_utils as ptu\n",
    "# import environments\n",
    "import envs.pomdp\n",
    "# import recurrent model-free RL (separate architecture)\n",
    "from policies.models.policy_rnn import ModelFreeOffPolicy_Separate_RNN as Policy_RNN\n",
    "# import the replay buffer\n",
    "from buffers.seq_replay_buffer_vanilla import SeqReplayBuffer\n",
    "from utils import helpers as utl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build a POMDP environment: Pendulum-V (only observe the velocity)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "cuda_id = 0 # -1 if using cpu\n",
    "ptu.set_gpu_mode(torch.cuda.is_available() and cuda_id >= 0, cuda_id)\n",
    "\n",
    "env_name = \"Pendulum-V-v0\"\n",
    "env = gym.make(env_name)\n",
    "max_trajectory_len = env._max_episode_steps\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "print(env, obs_dim, act_dim, max_trajectory_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TimeLimit<POMDPWrapper<TimeLimit<PendulumEnv<Pendulum-V-v0>>>>> 1 1 200\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build a recurent model-free RL agent: separate architecture, `lstm` encoder, `oar` policy input space, `td3` RL algorithm (context length set later)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "agent = Policy_RNN(\n",
    "            obs_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            state_dim=4,\n",
    "            encoder=\"lstm\",\n",
    "            algo=\"td3\",\n",
    "            action_embedding_size=8,\n",
    "            state_embedding_size=32,\n",
    "            reward_embedding_size=8,\n",
    "            rnn_hidden_size=128,\n",
    "            dqn_layers=[128, 128],\n",
    "            policy_layers=[128, 128],\n",
    "            lr=0.0003,\n",
    "            gamma=0.9,\n",
    "            tau=0.005,\n",
    "        ).to(ptu.device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define other training parameters such as context length and training frequency"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "num_updates_per_iter = 1.0 # training frequency\n",
    "sampled_seq_len = 64 # context length\n",
    "buffer_size = 1e6\n",
    "batch_size = 32\n",
    "\n",
    "num_iters = 150 \n",
    "num_init_rollouts_pool = 5\n",
    "num_rollouts_per_iter = 1\n",
    "total_rollouts = num_init_rollouts_pool + num_iters * num_rollouts_per_iter\n",
    "n_env_steps_total = max_trajectory_len * total_rollouts\n",
    "_n_env_steps_total = 0\n",
    "print(\"total env episodes\", total_rollouts, \"total env steps\", n_env_steps_total)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total env episodes 155 total env steps 31000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define key functions: collect rollouts and policy update"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    num_rollouts, \n",
    "    random_actions=False, \n",
    "    deterministic=False, \n",
    "    train_mode=True\n",
    "):\n",
    "    \"\"\"collect num_rollouts of trajectories in task and save into policy buffer\n",
    "    :param \n",
    "        random_actions: whether to use policy to sample actions, or randomly sample action space\n",
    "        deterministic: deterministic action selection?\n",
    "        train_mode: whether to train (stored to buffer) or test\n",
    "    \"\"\"\n",
    "    if not train_mode:\n",
    "        assert random_actions == False and deterministic == True\n",
    "\n",
    "    total_steps = 0\n",
    "    total_rewards = 0.0\n",
    "\n",
    "    for idx in range(num_rollouts):\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        obs = ptu.from_numpy(env.reset())\n",
    "        obs = obs.reshape(1, obs.shape[-1])\n",
    "        done_rollout = False\n",
    "\n",
    "        # get hidden state at timestep=0, None for mlp\n",
    "        action, reward, internal_state = agent.get_initial_info()\n",
    "\n",
    "        if train_mode:\n",
    "            # temporary storage\n",
    "            obs_list, act_list, rew_list, next_obs_list, term_list = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "\n",
    "        while not done_rollout:\n",
    "            if random_actions:\n",
    "                action = ptu.FloatTensor(\n",
    "                    [env.action_space.sample()]\n",
    "                )  # (1, A)\n",
    "            else:  \n",
    "                # policy takes hidden state as input for rnn, while takes obs for mlp\n",
    "                (action, _, _, _), internal_state = agent.act(\n",
    "                    prev_internal_state=internal_state,\n",
    "                    prev_action=action,\n",
    "                    reward=reward,\n",
    "                    obs=obs,\n",
    "                    deterministic=deterministic,\n",
    "                )\n",
    "            # observe reward and next obs (B=1, dim)\n",
    "            next_obs, reward, done, info = utl.env_step(\n",
    "                env, action.squeeze(dim=0)\n",
    "            )\n",
    "            done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "            \n",
    "            # update statistics\n",
    "            steps += 1\n",
    "            rewards += reward.item()\n",
    "\n",
    "            # early stopping env: such as rmdp, pomdp, generalize tasks. term ignores timeout\n",
    "            term = (\n",
    "                False\n",
    "                if \"TimeLimit.truncated\" in info\n",
    "                or steps >= max_trajectory_len\n",
    "                else done_rollout\n",
    "            )\n",
    "            \n",
    "            if train_mode:\n",
    "                # append tensors to temporary storage\n",
    "                obs_list.append(obs)  # (1, dim)\n",
    "                act_list.append(action)  # (1, dim)\n",
    "                rew_list.append(reward)  # (1, dim)\n",
    "                term_list.append(term)  # bool\n",
    "                next_obs_list.append(next_obs)  # (1, dim)\n",
    "            \n",
    "            # set: obs <- next_obs\n",
    "            obs = next_obs.clone()\n",
    "\n",
    "        if train_mode:\n",
    "            # add collected sequence to buffer\n",
    "            policy_storage.add_episode(\n",
    "                observations=ptu.get_numpy(torch.cat(obs_list, dim=0)),  # (L, dim)\n",
    "                actions=ptu.get_numpy(torch.cat(act_list, dim=0)),  # (L, dim)\n",
    "                rewards=ptu.get_numpy(torch.cat(rew_list, dim=0)),  # (L, dim)\n",
    "                terminals=np.array(term_list).reshape(-1, 1),  # (L, 1)\n",
    "                next_observations=ptu.get_numpy(\n",
    "                    torch.cat(next_obs_list, dim=0)\n",
    "                ),  # (L, dim)\n",
    "            )\n",
    "        print(\"Mode:\", \"Train\" if train_mode else \"Test\", \n",
    "             \"env_steps\", steps, \n",
    "             \"total rewards\", rewards)\n",
    "        total_steps += steps\n",
    "        total_rewards += rewards\n",
    "\n",
    "    if train_mode:\n",
    "        return total_steps\n",
    "    else:\n",
    "        return total_rewards / num_rollouts\n",
    "\n",
    "def update(num_updates):\n",
    "    rl_losses_agg = {}\n",
    "    # print(num_updates)\n",
    "    for update in range(num_updates):\n",
    "        # sample random RL batch: in transitions\n",
    "        batch = ptu.np_to_pytorch_batch(\n",
    "                    policy_storage.random_episodes(batch_size)\n",
    "                )\n",
    "        # RL update\n",
    "        rl_losses = agent.update(batch)\n",
    "\n",
    "        for k, v in rl_losses.items():\n",
    "            if update == 0:  # first iterate - create list\n",
    "                rl_losses_agg[k] = [v]\n",
    "            else:  # append values\n",
    "                rl_losses_agg[k].append(v)\n",
    "    # statistics\n",
    "    for k in rl_losses_agg:\n",
    "        rl_losses_agg[k] = np.mean(rl_losses_agg[k])\n",
    "    return rl_losses_agg\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Evaluate the agent: only costs < 20 min"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "policy_storage = SeqReplayBuffer(\n",
    "                max_replay_buffer_size=int(buffer_size),\n",
    "                observation_dim=obs_dim,\n",
    "                action_dim=act_dim,\n",
    "                sampled_seq_len=sampled_seq_len,\n",
    "                sample_weight_baseline=0.0,\n",
    "            )\n",
    "\n",
    "env_steps = collect_rollouts(num_rollouts=num_init_rollouts_pool, \n",
    "                             random_actions=True,\n",
    "                             train_mode=True\n",
    "                            )\n",
    "_n_env_steps_total += env_steps\n",
    "\n",
    "# evaluation parameters\n",
    "last_eval_num_iters = 0\n",
    "log_interval = 5\n",
    "eval_num_rollouts = 10\n",
    "learning_curve = {\n",
    "    'x': [],\n",
    "    'y': [],\n",
    "}\n",
    "\n",
    "while _n_env_steps_total < n_env_steps_total:\n",
    "    \n",
    "    env_steps = collect_rollouts(num_rollouts=num_rollouts_per_iter, \n",
    "                                train_mode=True\n",
    "                                )\n",
    "    _n_env_steps_total += env_steps\n",
    "\n",
    "    train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "\n",
    "    current_num_iters = _n_env_steps_total // (\n",
    "                            num_rollouts_per_iter * max_trajectory_len)\n",
    "    if (current_num_iters != last_eval_num_iters\n",
    "        and current_num_iters % log_interval == 0):\n",
    "        last_eval_num_iters = current_num_iters\n",
    "        average_returns = collect_rollouts(\n",
    "                                num_rollouts=eval_num_rollouts, \n",
    "                                train_mode=False, \n",
    "                                random_actions=False, \n",
    "                                deterministic=True\n",
    "                            )\n",
    "        learning_curve['x'].append(_n_env_steps_total)\n",
    "        learning_curve['y'].append(average_returns)\n",
    "        print(_n_env_steps_total, average_returns)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "buffer RAM usage: 0.02 GB\n",
      "Mode: Train env_steps 200 total rewards -1204.059592783451\n",
      "Mode: Train env_steps 200 total rewards -989.1774921696633\n",
      "Mode: Train env_steps 200 total rewards -1476.445612192154\n",
      "Mode: Train env_steps 200 total rewards -1158.6363698244095\n",
      "Mode: Train env_steps 200 total rewards -1295.1461539268494\n",
      "Mode: Train env_steps 200 total rewards -1190.5720336437225\n",
      "Mode: Train env_steps 200 total rewards -1486.67218708992\n",
      "Mode: Train env_steps 200 total rewards -1716.249975681305\n",
      "Mode: Train env_steps 200 total rewards -1692.9510417878628\n",
      "Mode: Train env_steps 200 total rewards -1502.1382110118866\n",
      "Mode: Test env_steps 200 total rewards -1527.799259185791\n",
      "Mode: Test env_steps 200 total rewards -1519.9005613327026\n",
      "Mode: Test env_steps 200 total rewards -1133.674363905564\n",
      "Mode: Test env_steps 200 total rewards -1507.8130168914795\n",
      "Mode: Test env_steps 200 total rewards -1353.6502655728254\n",
      "Mode: Test env_steps 200 total rewards -1483.3580957651138\n",
      "Mode: Test env_steps 200 total rewards -1496.8963022232056\n",
      "Mode: Test env_steps 200 total rewards -1529.4321298599243\n",
      "Mode: Test env_steps 200 total rewards -1515.145750761032\n",
      "Mode: Test env_steps 200 total rewards -1384.321657601744\n",
      "2000 -1445.1991403099382\n",
      "Mode: Train env_steps 200 total rewards -1522.1170699596405\n",
      "Mode: Train env_steps 200 total rewards -1546.110505104065\n",
      "Mode: Train env_steps 200 total rewards -1499.7653061151505\n",
      "Mode: Train env_steps 200 total rewards -1497.1999181509018\n",
      "Mode: Train env_steps 200 total rewards -1522.6442246437073\n",
      "Mode: Test env_steps 200 total rewards -1327.8945291042328\n",
      "Mode: Test env_steps 200 total rewards -1505.104232788086\n",
      "Mode: Test env_steps 200 total rewards -1325.6553157567978\n",
      "Mode: Test env_steps 200 total rewards -1337.9899653196335\n",
      "Mode: Test env_steps 200 total rewards -1332.1156732738018\n",
      "Mode: Test env_steps 200 total rewards -1484.3709874153137\n",
      "Mode: Test env_steps 200 total rewards -1347.7305238246918\n",
      "Mode: Test env_steps 200 total rewards -1113.5202559269965\n",
      "Mode: Test env_steps 200 total rewards -1181.7054929723963\n",
      "Mode: Test env_steps 200 total rewards -1331.3508676588535\n",
      "3000 -1328.7437844040803\n",
      "Mode: Train env_steps 200 total rewards -1502.8507390022278\n",
      "Mode: Train env_steps 200 total rewards -1406.9906678199768\n",
      "Mode: Train env_steps 200 total rewards -1343.264373600483\n",
      "Mode: Train env_steps 200 total rewards -1660.3930011987686\n",
      "Mode: Train env_steps 200 total rewards -1396.2767609655857\n",
      "Mode: Test env_steps 200 total rewards -1597.4195498228073\n",
      "Mode: Test env_steps 200 total rewards -1563.963100463152\n",
      "Mode: Test env_steps 200 total rewards -1296.2775453031063\n",
      "Mode: Test env_steps 200 total rewards -1616.8411464691162\n",
      "Mode: Test env_steps 200 total rewards -1423.2955019772053\n",
      "Mode: Test env_steps 200 total rewards -1405.6913492064923\n",
      "Mode: Test env_steps 200 total rewards -1414.35410040617\n",
      "Mode: Test env_steps 200 total rewards -1547.722985625267\n",
      "Mode: Test env_steps 200 total rewards -1678.1708958148956\n",
      "Mode: Test env_steps 200 total rewards -1629.2236394286156\n",
      "4000 -1517.2959814516828\n",
      "Mode: Train env_steps 200 total rewards -1508.2409524917603\n",
      "Mode: Train env_steps 200 total rewards -896.5191936641932\n",
      "Mode: Train env_steps 200 total rewards -526.1463507041335\n",
      "Mode: Train env_steps 200 total rewards -711.0142463892698\n",
      "Mode: Train env_steps 200 total rewards -519.3354113176465\n",
      "Mode: Test env_steps 200 total rewards -489.4628938399255\n",
      "Mode: Test env_steps 200 total rewards -530.5279819592834\n",
      "Mode: Test env_steps 200 total rewards -521.2647749520838\n",
      "Mode: Test env_steps 200 total rewards -528.6125348322093\n",
      "Mode: Test env_steps 200 total rewards -512.1036762222648\n",
      "Mode: Test env_steps 200 total rewards -530.6729961037636\n",
      "Mode: Test env_steps 200 total rewards -527.736331909895\n",
      "Mode: Test env_steps 200 total rewards -527.2781273797154\n",
      "Mode: Test env_steps 200 total rewards -654.5962308179587\n",
      "Mode: Test env_steps 200 total rewards -527.0040097758174\n",
      "5000 -534.9259557792917\n",
      "Mode: Train env_steps 200 total rewards -636.85912425071\n",
      "Mode: Train env_steps 200 total rewards -1204.554298080504\n",
      "Mode: Train env_steps 200 total rewards -648.07353939116\n",
      "Mode: Train env_steps 200 total rewards -613.5016740635037\n",
      "Mode: Train env_steps 200 total rewards -520.9085429552943\n",
      "Mode: Test env_steps 200 total rewards -554.3990800231695\n",
      "Mode: Test env_steps 200 total rewards -771.5601937174797\n",
      "Mode: Test env_steps 200 total rewards -553.90392551478\n",
      "Mode: Test env_steps 200 total rewards -770.1281457021832\n",
      "Mode: Test env_steps 200 total rewards -600.4290646314621\n",
      "Mode: Test env_steps 200 total rewards -611.9977835714817\n",
      "Mode: Test env_steps 200 total rewards -719.6285204961896\n",
      "Mode: Test env_steps 200 total rewards -765.9643004983664\n",
      "Mode: Test env_steps 200 total rewards -549.8600461706519\n",
      "Mode: Test env_steps 200 total rewards -522.6526645515114\n",
      "6000 -642.0523724877276\n",
      "Mode: Train env_steps 200 total rewards -636.5371189061552\n",
      "Mode: Train env_steps 200 total rewards -483.75789686013013\n",
      "Mode: Train env_steps 200 total rewards -658.0404519215226\n",
      "Mode: Train env_steps 200 total rewards -750.9774870760739\n",
      "Mode: Train env_steps 200 total rewards -139.79548785556108\n",
      "Mode: Test env_steps 200 total rewards -391.3397971680388\n",
      "Mode: Test env_steps 200 total rewards -399.37121712975204\n",
      "Mode: Test env_steps 200 total rewards -507.36942733451724\n",
      "Mode: Test env_steps 200 total rewards -516.9214701503515\n",
      "Mode: Test env_steps 200 total rewards -415.59001878648996\n",
      "Mode: Test env_steps 200 total rewards -512.0226002912968\n",
      "Mode: Test env_steps 200 total rewards -578.9707840736955\n",
      "Mode: Test env_steps 200 total rewards -510.9068979471922\n",
      "Mode: Test env_steps 200 total rewards -642.8351273015141\n",
      "Mode: Test env_steps 200 total rewards -437.2822688780725\n",
      "7000 -491.26096090609207\n",
      "Mode: Train env_steps 200 total rewards -489.29174616001546\n",
      "Mode: Train env_steps 200 total rewards -416.59107636101544\n",
      "Mode: Train env_steps 200 total rewards -509.0554213821888\n",
      "Mode: Train env_steps 200 total rewards -630.6913189487532\n",
      "Mode: Train env_steps 200 total rewards -390.96725356345996\n",
      "Mode: Test env_steps 200 total rewards -638.4102736525238\n",
      "Mode: Test env_steps 200 total rewards -534.0876863487065\n",
      "Mode: Test env_steps 200 total rewards -525.5858491696417\n",
      "Mode: Test env_steps 200 total rewards -527.8903956897557\n",
      "Mode: Test env_steps 200 total rewards -526.8521353416145\n",
      "Mode: Test env_steps 200 total rewards -522.5951464474201\n",
      "Mode: Test env_steps 200 total rewards -524.4304926805198\n",
      "Mode: Test env_steps 200 total rewards -517.2932330146432\n",
      "Mode: Test env_steps 200 total rewards -530.9962109923363\n",
      "Mode: Test env_steps 200 total rewards -522.769685909152\n",
      "8000 -537.0911109246314\n",
      "Mode: Train env_steps 200 total rewards -666.7655238062143\n",
      "Mode: Train env_steps 200 total rewards -585.4233639389277\n",
      "Mode: Train env_steps 200 total rewards -259.5432086917572\n",
      "Mode: Train env_steps 200 total rewards -263.43320117751136\n",
      "Mode: Train env_steps 200 total rewards -198.37094931863248\n",
      "Mode: Test env_steps 200 total rewards -261.12575925979763\n",
      "Mode: Test env_steps 200 total rewards -265.5147065958008\n",
      "Mode: Test env_steps 200 total rewards -7.835292627103627\n",
      "Mode: Test env_steps 200 total rewards -399.4131638677791\n",
      "Mode: Test env_steps 200 total rewards -134.98240291327238\n",
      "Mode: Test env_steps 200 total rewards -1497.889838218689\n",
      "Mode: Test env_steps 200 total rewards -135.86132941627875\n",
      "Mode: Test env_steps 200 total rewards -269.1106326263398\n",
      "Mode: Test env_steps 200 total rewards -134.26537928637117\n",
      "Mode: Test env_steps 200 total rewards -374.1115903733298\n",
      "9000 -348.0110095184762\n",
      "Mode: Train env_steps 200 total rewards -135.44967245310545\n",
      "Mode: Train env_steps 200 total rewards -120.11081185331568\n",
      "Mode: Train env_steps 200 total rewards -588.2196611557156\n",
      "Mode: Train env_steps 200 total rewards -284.84660888370126\n",
      "Mode: Train env_steps 200 total rewards -1509.706886768341\n",
      "Mode: Test env_steps 200 total rewards -120.35460204951232\n",
      "Mode: Test env_steps 200 total rewards -267.6110439975746\n",
      "Mode: Test env_steps 200 total rewards -126.26122943215887\n",
      "Mode: Test env_steps 200 total rewards -125.6453795976995\n",
      "Mode: Test env_steps 200 total rewards -129.2382336515002\n",
      "Mode: Test env_steps 200 total rewards -400.5788985872641\n",
      "Mode: Test env_steps 200 total rewards -126.79820043593645\n",
      "Mode: Test env_steps 200 total rewards -254.02042074594647\n",
      "Mode: Test env_steps 200 total rewards -267.5792810758576\n",
      "Mode: Test env_steps 200 total rewards -132.58660741063068\n",
      "10000 -195.06738969840808\n",
      "Mode: Train env_steps 200 total rewards -130.67684872521204\n",
      "Mode: Train env_steps 200 total rewards -125.88542355428217\n",
      "Mode: Train env_steps 200 total rewards -121.93960176120163\n",
      "Mode: Train env_steps 200 total rewards -126.6532150455605\n",
      "Mode: Train env_steps 200 total rewards -127.43772032592096\n",
      "Mode: Test env_steps 200 total rewards -126.4726840476942\n",
      "Mode: Test env_steps 200 total rewards -292.9569091968151\n",
      "Mode: Test env_steps 200 total rewards -129.2508988091795\n",
      "Mode: Test env_steps 200 total rewards -136.39653292084404\n",
      "Mode: Test env_steps 200 total rewards -128.4768375010317\n",
      "Mode: Test env_steps 200 total rewards -0.9382629043466295\n",
      "Mode: Test env_steps 200 total rewards -278.59592088828504\n",
      "Mode: Test env_steps 200 total rewards -127.0598166215641\n",
      "Mode: Test env_steps 200 total rewards -127.55167253677791\n",
      "Mode: Test env_steps 200 total rewards -4.852087763618329\n",
      "11000 -135.25516231901565\n",
      "Mode: Train env_steps 200 total rewards -128.49951019315995\n",
      "Mode: Train env_steps 200 total rewards -409.67553202129784\n",
      "Mode: Train env_steps 200 total rewards -298.3724636931438\n",
      "Mode: Train env_steps 200 total rewards -294.2433500648476\n",
      "Mode: Train env_steps 200 total rewards -398.66701237819234\n",
      "Mode: Test env_steps 200 total rewards -124.28654096552054\n",
      "Mode: Test env_steps 200 total rewards -131.83797391915869\n",
      "Mode: Test env_steps 200 total rewards -131.73685164382914\n",
      "Mode: Test env_steps 200 total rewards -122.80770650089835\n",
      "Mode: Test env_steps 200 total rewards -256.6840043792472\n",
      "Mode: Test env_steps 200 total rewards -0.8581397420894064\n",
      "Mode: Test env_steps 200 total rewards -127.93062768180971\n",
      "Mode: Test env_steps 200 total rewards -117.54234447127237\n",
      "Mode: Test env_steps 200 total rewards -245.19505203775043\n",
      "Mode: Test env_steps 200 total rewards -390.3177622448784\n",
      "12000 -164.91970035864543\n",
      "Mode: Train env_steps 200 total rewards -119.74055544819566\n",
      "Mode: Train env_steps 200 total rewards -125.3588029865059\n",
      "Mode: Train env_steps 200 total rewards -383.75023366248934\n",
      "Mode: Train env_steps 200 total rewards -277.730409866781\n",
      "Mode: Train env_steps 200 total rewards -268.0766422232846\n",
      "Mode: Test env_steps 200 total rewards -121.39812675974099\n",
      "Mode: Test env_steps 200 total rewards -250.1152592622093\n",
      "Mode: Test env_steps 200 total rewards -124.38505058723968\n",
      "Mode: Test env_steps 200 total rewards -118.71435798968014\n",
      "Mode: Test env_steps 200 total rewards -243.78981299410225\n",
      "Mode: Test env_steps 200 total rewards -129.8467649478407\n",
      "Mode: Test env_steps 200 total rewards -0.9133762145211222\n",
      "Mode: Test env_steps 200 total rewards -126.15278096000839\n",
      "Mode: Test env_steps 200 total rewards -119.49253472717828\n",
      "Mode: Test env_steps 200 total rewards -131.1635592887178\n",
      "13000 -136.59716237312387\n",
      "Mode: Train env_steps 200 total rewards -130.53151915289345\n",
      "Mode: Train env_steps 200 total rewards -379.13928919882164\n",
      "Mode: Train env_steps 200 total rewards -128.99929263646482\n",
      "Mode: Train env_steps 200 total rewards -122.47591581195593\n",
      "Mode: Train env_steps 200 total rewards -122.67732962686568\n",
      "Mode: Test env_steps 200 total rewards -131.0189595722768\n",
      "Mode: Test env_steps 200 total rewards -134.2590572948102\n",
      "Mode: Test env_steps 200 total rewards -119.8275818855036\n",
      "Mode: Test env_steps 200 total rewards -130.88440640344197\n",
      "Mode: Test env_steps 200 total rewards -125.46393105335301\n",
      "Mode: Test env_steps 200 total rewards -290.45477236888837\n",
      "Mode: Test env_steps 200 total rewards -129.75853787749656\n",
      "Mode: Test env_steps 200 total rewards -129.54066526400857\n",
      "Mode: Test env_steps 200 total rewards -349.85848432034254\n",
      "Mode: Test env_steps 200 total rewards -127.17198752681725\n",
      "14000 -166.8238383566939\n",
      "Mode: Train env_steps 200 total rewards -132.10711615649052\n",
      "Mode: Train env_steps 200 total rewards -256.8173265758669\n",
      "Mode: Train env_steps 200 total rewards -127.86162062268704\n",
      "Mode: Train env_steps 200 total rewards -244.84809440476238\n",
      "Mode: Train env_steps 200 total rewards -312.3251590181608\n",
      "Mode: Test env_steps 200 total rewards -119.63847100461135\n",
      "Mode: Test env_steps 200 total rewards -127.62217518611578\n",
      "Mode: Test env_steps 200 total rewards -257.2846976958099\n",
      "Mode: Test env_steps 200 total rewards -357.8439167421311\n",
      "Mode: Test env_steps 200 total rewards -251.47988980187802\n",
      "Mode: Test env_steps 200 total rewards -252.54562737955712\n",
      "Mode: Test env_steps 200 total rewards -122.33756510604871\n",
      "Mode: Test env_steps 200 total rewards -131.27657976921182\n",
      "Mode: Test env_steps 200 total rewards -400.9677457814687\n",
      "Mode: Test env_steps 200 total rewards -1.4743770357454196\n",
      "15000 -202.2471045502578\n",
      "Mode: Train env_steps 200 total rewards -362.6342863460304\n",
      "Mode: Train env_steps 200 total rewards -261.4412091865379\n",
      "Mode: Train env_steps 200 total rewards -130.31228977557475\n",
      "Mode: Train env_steps 200 total rewards -246.5588745214045\n",
      "Mode: Train env_steps 200 total rewards -6.4433038181159645\n",
      "Mode: Test env_steps 200 total rewards -128.18568862089887\n",
      "Mode: Test env_steps 200 total rewards -254.63203795196023\n",
      "Mode: Test env_steps 200 total rewards -135.15028415620327\n",
      "Mode: Test env_steps 200 total rewards -128.667353752302\n",
      "Mode: Test env_steps 200 total rewards -137.3656748905778\n",
      "Mode: Test env_steps 200 total rewards -129.97955457121134\n",
      "Mode: Test env_steps 200 total rewards -134.14977611973882\n",
      "Mode: Test env_steps 200 total rewards -244.77459020912647\n",
      "Mode: Test env_steps 200 total rewards -7.824678143486381\n",
      "Mode: Test env_steps 200 total rewards -135.25964403152466\n",
      "16000 -143.598928244703\n",
      "Mode: Train env_steps 200 total rewards -121.32234912895365\n",
      "Mode: Train env_steps 200 total rewards -132.6226024988573\n",
      "Mode: Train env_steps 200 total rewards -252.0155453520565\n",
      "Mode: Train env_steps 200 total rewards -126.44306003744714\n",
      "Mode: Train env_steps 200 total rewards -262.205412579875\n",
      "Mode: Test env_steps 200 total rewards -117.13862014511449\n",
      "Mode: Test env_steps 200 total rewards -129.70662306129344\n",
      "Mode: Test env_steps 200 total rewards -274.4145622906369\n",
      "Mode: Test env_steps 200 total rewards -128.77530803742874\n",
      "Mode: Test env_steps 200 total rewards -3.508734864121076\n",
      "Mode: Test env_steps 200 total rewards -132.35854013591995\n",
      "Mode: Test env_steps 200 total rewards -127.34810837316415\n",
      "Mode: Test env_steps 200 total rewards -335.83958516085085\n",
      "Mode: Test env_steps 200 total rewards -117.30884116277669\n",
      "Mode: Test env_steps 200 total rewards -128.42445322953262\n",
      "17000 -149.4823376460839\n",
      "Mode: Train env_steps 200 total rewards -249.17723875812953\n",
      "Mode: Train env_steps 200 total rewards -136.19914626330137\n",
      "Mode: Train env_steps 200 total rewards -0.11309282757792971\n",
      "Mode: Train env_steps 200 total rewards -349.8059202595359\n",
      "Mode: Train env_steps 200 total rewards -136.2089309138246\n",
      "Mode: Test env_steps 200 total rewards -250.22573195467703\n",
      "Mode: Test env_steps 200 total rewards -130.18660731473938\n",
      "Mode: Test env_steps 200 total rewards -407.4748675771989\n",
      "Mode: Test env_steps 200 total rewards -123.81802814314142\n",
      "Mode: Test env_steps 200 total rewards -244.62673494033515\n",
      "Mode: Test env_steps 200 total rewards -119.01994826365262\n",
      "Mode: Test env_steps 200 total rewards -133.66823041811585\n",
      "Mode: Test env_steps 200 total rewards -245.75636586360633\n",
      "Mode: Test env_steps 200 total rewards -123.86068900930695\n",
      "Mode: Test env_steps 200 total rewards -131.27882482018322\n",
      "18000 -190.99160283049568\n",
      "Mode: Train env_steps 200 total rewards -127.36306210799376\n",
      "Mode: Train env_steps 200 total rewards -130.6829265653505\n",
      "Mode: Train env_steps 200 total rewards -382.24824262061156\n",
      "Mode: Train env_steps 200 total rewards -120.59308872569818\n",
      "Mode: Train env_steps 200 total rewards -247.3790498850285\n",
      "Mode: Test env_steps 200 total rewards -130.45041601388948\n",
      "Mode: Test env_steps 200 total rewards -125.9475084610749\n",
      "Mode: Test env_steps 200 total rewards -131.4618440955528\n",
      "Mode: Test env_steps 200 total rewards -269.0013475093874\n",
      "Mode: Test env_steps 200 total rewards -128.61137787206098\n",
      "Mode: Test env_steps 200 total rewards -131.46151708887191\n",
      "Mode: Test env_steps 200 total rewards -272.20577986567514\n",
      "Mode: Test env_steps 200 total rewards -247.2670529182069\n",
      "Mode: Test env_steps 200 total rewards -399.35839442466386\n",
      "Mode: Test env_steps 200 total rewards -130.30424671189394\n",
      "19000 -196.60694849612773\n",
      "Mode: Train env_steps 200 total rewards -131.46077986550517\n",
      "Mode: Train env_steps 200 total rewards -129.8015597909689\n",
      "Mode: Train env_steps 200 total rewards -503.66162280132994\n",
      "Mode: Train env_steps 200 total rewards -250.42238160556008\n",
      "Mode: Train env_steps 200 total rewards -135.24958999082446\n",
      "Mode: Test env_steps 200 total rewards -260.0345336853061\n",
      "Mode: Test env_steps 200 total rewards -247.35475993761793\n",
      "Mode: Test env_steps 200 total rewards -392.4793661334552\n",
      "Mode: Test env_steps 200 total rewards -267.0718862583162\n",
      "Mode: Test env_steps 200 total rewards -258.3940580005292\n",
      "Mode: Test env_steps 200 total rewards -4.229895625030622\n",
      "Mode: Test env_steps 200 total rewards -2.4437960271607153\n",
      "Mode: Test env_steps 200 total rewards -132.3561574909836\n",
      "Mode: Test env_steps 200 total rewards -253.3267279600259\n",
      "Mode: Test env_steps 200 total rewards -133.2784153984976\n",
      "20000 -195.09695965169232\n",
      "Mode: Train env_steps 200 total rewards -257.4457959874999\n",
      "Mode: Train env_steps 200 total rewards -122.97991019329902\n",
      "Mode: Train env_steps 200 total rewards -434.8894098699093\n",
      "Mode: Train env_steps 200 total rewards -411.132038122043\n",
      "Mode: Train env_steps 200 total rewards -2.2382346628364758\n",
      "Mode: Test env_steps 200 total rewards -259.35440591632505\n",
      "Mode: Test env_steps 200 total rewards -128.50231397691095\n",
      "Mode: Test env_steps 200 total rewards -129.0294535429166\n",
      "Mode: Test env_steps 200 total rewards -257.08948847383726\n",
      "Mode: Test env_steps 200 total rewards -1.9071165944769746\n",
      "Mode: Test env_steps 200 total rewards -0.4021077781217173\n",
      "Mode: Test env_steps 200 total rewards -117.35778525004571\n",
      "Mode: Test env_steps 200 total rewards -129.74248643833562\n",
      "Mode: Test env_steps 200 total rewards -257.24292338400846\n",
      "Mode: Test env_steps 200 total rewards -131.2033825230319\n",
      "21000 -141.18314638780103\n",
      "Mode: Train env_steps 200 total rewards -136.1106141936034\n",
      "Mode: Train env_steps 200 total rewards -124.77838193829666\n",
      "Mode: Train env_steps 200 total rewards -132.488145424315\n",
      "Mode: Train env_steps 200 total rewards -119.93253558172728\n",
      "Mode: Train env_steps 200 total rewards -131.8471141194459\n",
      "Mode: Test env_steps 200 total rewards -134.9782147333026\n",
      "Mode: Test env_steps 200 total rewards -261.169426901135\n",
      "Mode: Test env_steps 200 total rewards -371.7737183123827\n",
      "Mode: Test env_steps 200 total rewards -409.4373374916613\n",
      "Mode: Test env_steps 200 total rewards -130.04347233075532\n",
      "Mode: Test env_steps 200 total rewards -251.5282223932445\n",
      "Mode: Test env_steps 200 total rewards -119.57643118570559\n",
      "Mode: Test env_steps 200 total rewards -130.13410744362045\n",
      "Mode: Test env_steps 200 total rewards -242.0278536007536\n",
      "Mode: Test env_steps 200 total rewards -3.2553650510599255\n",
      "22000 -205.3924149443621\n",
      "Mode: Train env_steps 200 total rewards -261.60932337492704\n",
      "Mode: Train env_steps 200 total rewards -256.5225995981018\n",
      "Mode: Train env_steps 200 total rewards -132.85395933969994\n",
      "Mode: Train env_steps 200 total rewards -133.56817623705138\n",
      "Mode: Train env_steps 200 total rewards -248.61654692026787\n",
      "Mode: Test env_steps 200 total rewards -376.59752772955653\n",
      "Mode: Test env_steps 200 total rewards -134.4399830829352\n",
      "Mode: Test env_steps 200 total rewards -134.78479183278978\n",
      "Mode: Test env_steps 200 total rewards -132.5015555229038\n",
      "Mode: Test env_steps 200 total rewards -136.48942646756768\n",
      "Mode: Test env_steps 200 total rewards -128.9631020313609\n",
      "Mode: Test env_steps 200 total rewards -2.468763148965081\n",
      "Mode: Test env_steps 200 total rewards -127.05833978322335\n",
      "Mode: Test env_steps 200 total rewards -135.9549485463649\n",
      "Mode: Test env_steps 200 total rewards -322.98864406347275\n",
      "23000 -163.224708220914\n",
      "Mode: Train env_steps 200 total rewards -124.8159738369286\n",
      "Mode: Train env_steps 200 total rewards -122.3691240940243\n",
      "Mode: Train env_steps 200 total rewards -288.53548070140823\n",
      "Mode: Train env_steps 200 total rewards -131.6395241394639\n",
      "Mode: Train env_steps 200 total rewards -540.1620760088263\n",
      "Mode: Test env_steps 200 total rewards -133.21957878675312\n",
      "Mode: Test env_steps 200 total rewards -405.02124067768455\n",
      "Mode: Test env_steps 200 total rewards -322.8707986716181\n",
      "Mode: Test env_steps 200 total rewards -8.822469983249903\n",
      "Mode: Test env_steps 200 total rewards -127.25688949786127\n",
      "Mode: Test env_steps 200 total rewards -274.35081420093775\n",
      "Mode: Test env_steps 200 total rewards -122.53521027229726\n",
      "Mode: Test env_steps 200 total rewards -130.4988274630159\n",
      "Mode: Test env_steps 200 total rewards -271.97348071448505\n",
      "Mode: Test env_steps 200 total rewards -139.74689017236233\n",
      "24000 -193.62962004402652\n",
      "Mode: Train env_steps 200 total rewards -131.31790862604976\n",
      "Mode: Train env_steps 200 total rewards -276.97604647651315\n",
      "Mode: Train env_steps 200 total rewards -135.85259454324841\n",
      "Mode: Train env_steps 200 total rewards -130.65943918656558\n",
      "Mode: Train env_steps 200 total rewards -248.0910578770563\n",
      "Mode: Test env_steps 200 total rewards -132.75350271817297\n",
      "Mode: Test env_steps 200 total rewards -254.22568294219673\n",
      "Mode: Test env_steps 200 total rewards -244.58955243974924\n",
      "Mode: Test env_steps 200 total rewards -127.681380729191\n",
      "Mode: Test env_steps 200 total rewards -138.24134643375874\n",
      "Mode: Test env_steps 200 total rewards -137.31484142318368\n",
      "Mode: Test env_steps 200 total rewards -133.05783389415592\n",
      "Mode: Test env_steps 200 total rewards -146.32316165417433\n",
      "Mode: Test env_steps 200 total rewards -135.2591582145542\n",
      "Mode: Test env_steps 200 total rewards -315.4043061719276\n",
      "25000 -176.48507666210645\n",
      "Mode: Train env_steps 200 total rewards -133.82533819042146\n",
      "Mode: Train env_steps 200 total rewards -134.8162428275682\n",
      "Mode: Train env_steps 200 total rewards -244.32623816700652\n",
      "Mode: Train env_steps 200 total rewards -3.778044021455571\n",
      "Mode: Train env_steps 200 total rewards -128.83338074281346\n",
      "Mode: Test env_steps 200 total rewards -127.93238051841035\n",
      "Mode: Test env_steps 200 total rewards -1.8821766900655348\n",
      "Mode: Test env_steps 200 total rewards -255.20747461158317\n",
      "Mode: Test env_steps 200 total rewards -132.93583739502355\n",
      "Mode: Test env_steps 200 total rewards -136.86306627717568\n",
      "Mode: Test env_steps 200 total rewards -129.44380271527916\n",
      "Mode: Test env_steps 200 total rewards -128.46021447662497\n",
      "Mode: Test env_steps 200 total rewards -263.654817867995\n",
      "Mode: Test env_steps 200 total rewards -250.00497410772368\n",
      "Mode: Test env_steps 200 total rewards -130.34925560402917\n",
      "26000 -155.67340002639102\n",
      "Mode: Train env_steps 200 total rewards -265.088537671254\n",
      "Mode: Train env_steps 200 total rewards -245.2897521123523\n",
      "Mode: Train env_steps 200 total rewards -135.54659879184328\n",
      "Mode: Train env_steps 200 total rewards -134.20274002745282\n",
      "Mode: Train env_steps 200 total rewards -132.49634827533737\n",
      "Mode: Test env_steps 200 total rewards -0.9372565113008022\n",
      "Mode: Test env_steps 200 total rewards -133.854406661776\n",
      "Mode: Test env_steps 200 total rewards -283.06745217135176\n",
      "Mode: Test env_steps 200 total rewards -135.53649241034873\n",
      "Mode: Test env_steps 200 total rewards -5.232758851154358\n",
      "Mode: Test env_steps 200 total rewards -128.73079440451693\n",
      "Mode: Test env_steps 200 total rewards -2.898283079266548\n",
      "Mode: Test env_steps 200 total rewards -122.33216101350263\n",
      "Mode: Test env_steps 200 total rewards -135.48738240776584\n",
      "Mode: Test env_steps 200 total rewards -135.19188347714953\n",
      "27000 -108.32688709881332\n",
      "Mode: Train env_steps 200 total rewards -127.93599926494062\n",
      "Mode: Train env_steps 200 total rewards -2.1926580952713266\n",
      "Mode: Train env_steps 200 total rewards -130.47707805549726\n",
      "Mode: Train env_steps 200 total rewards -261.9311791844666\n",
      "Mode: Train env_steps 200 total rewards -134.26524320803583\n",
      "Mode: Test env_steps 200 total rewards -128.5385453570634\n",
      "Mode: Test env_steps 200 total rewards -130.8503375891596\n",
      "Mode: Test env_steps 200 total rewards -127.71351309679449\n",
      "Mode: Test env_steps 200 total rewards -134.82161587662995\n",
      "Mode: Test env_steps 200 total rewards -262.1023752912879\n",
      "Mode: Test env_steps 200 total rewards -257.3817347101867\n",
      "Mode: Test env_steps 200 total rewards -130.11529196612537\n",
      "Mode: Test env_steps 200 total rewards -260.2616065284237\n",
      "Mode: Test env_steps 200 total rewards -132.16901091020554\n",
      "Mode: Test env_steps 200 total rewards -413.4033093829639\n",
      "28000 -197.73573407088406\n",
      "Mode: Train env_steps 200 total rewards -135.30749876610935\n",
      "Mode: Train env_steps 200 total rewards -116.55253962869756\n",
      "Mode: Train env_steps 200 total rewards -134.54131290875375\n",
      "Mode: Train env_steps 200 total rewards -130.63369534350932\n",
      "Mode: Train env_steps 200 total rewards -132.36800280958414\n",
      "Mode: Test env_steps 200 total rewards -135.59028842113912\n",
      "Mode: Test env_steps 200 total rewards -269.0078064724803\n",
      "Mode: Test env_steps 200 total rewards -124.86730461847037\n",
      "Mode: Test env_steps 200 total rewards -118.95745547581464\n",
      "Mode: Test env_steps 200 total rewards -253.15683409012854\n",
      "Mode: Test env_steps 200 total rewards -134.94601299241185\n",
      "Mode: Test env_steps 200 total rewards -129.89817111380398\n",
      "Mode: Test env_steps 200 total rewards -120.70617686770856\n",
      "Mode: Test env_steps 200 total rewards -4.725844678469002\n",
      "Mode: Test env_steps 200 total rewards -244.49303482752293\n",
      "29000 -153.63489295579492\n",
      "Mode: Train env_steps 200 total rewards -248.73104539886117\n",
      "Mode: Train env_steps 200 total rewards -374.026931877248\n",
      "Mode: Train env_steps 200 total rewards -136.52694238815457\n",
      "Mode: Train env_steps 200 total rewards -131.49138005502755\n",
      "Mode: Train env_steps 200 total rewards -399.39796920493245\n",
      "Mode: Test env_steps 200 total rewards -133.46671684039757\n",
      "Mode: Test env_steps 200 total rewards -247.99968862859532\n",
      "Mode: Test env_steps 200 total rewards -385.91397424787283\n",
      "Mode: Test env_steps 200 total rewards -252.42013315716758\n",
      "Mode: Test env_steps 200 total rewards -368.19159417646006\n",
      "Mode: Test env_steps 200 total rewards -136.13054853165522\n",
      "Mode: Test env_steps 200 total rewards -140.6763490885496\n",
      "Mode: Test env_steps 200 total rewards -133.86698543466628\n",
      "Mode: Test env_steps 200 total rewards -267.0093674957752\n",
      "Mode: Test env_steps 200 total rewards -6.7753472821787\n",
      "30000 -207.24507048833183\n",
      "Mode: Train env_steps 200 total rewards -138.29107024520636\n",
      "Mode: Train env_steps 200 total rewards -8.058146432042122\n",
      "Mode: Train env_steps 200 total rewards -124.61375629832037\n",
      "Mode: Train env_steps 200 total rewards -412.0572008565068\n",
      "Mode: Train env_steps 200 total rewards -121.52573706582189\n",
      "Mode: Test env_steps 200 total rewards -134.9321626186138\n",
      "Mode: Test env_steps 200 total rewards -119.70107615774032\n",
      "Mode: Test env_steps 200 total rewards -278.25727599067613\n",
      "Mode: Test env_steps 200 total rewards -128.9262896461878\n",
      "Mode: Test env_steps 200 total rewards -134.29476902261376\n",
      "Mode: Test env_steps 200 total rewards -413.7742978744209\n",
      "Mode: Test env_steps 200 total rewards -298.71761231380515\n",
      "Mode: Test env_steps 200 total rewards -134.67517527053133\n",
      "Mode: Test env_steps 200 total rewards -255.7470046284143\n",
      "Mode: Test env_steps 200 total rewards -139.58206834830344\n",
      "31000 -203.8607731871307\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "!pip uninstall dataclass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Skipping dataclass as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Draw the learning curve"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(learning_curve[\"x\"], learning_curve[\"y\"])\n",
    "plt.xlabel(\"env steps\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'typing' has no attribute '_ClassVar'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-10614dbdeb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.plot(learning_curve[\"x\"], learning_curve[\"y\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plt.xlabel(\"env steps\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.ylabel(\"return\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martist\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/contour.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_bases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMouseButton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from matplotlib import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_tools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     tight_bbox, transforms, widgets, get_backend, is_interactive, rcParams)\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/textpath.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_text_helpers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFontProperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_font\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mft2font\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOAD_NO_HINTING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOAD_TARGET_LIGHT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/matplotlib/_text_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m LayoutItem = dataclasses.make_dataclass(\n\u001b[0;32m---> 12\u001b[0;31m     \"LayoutItem\", [\"char\", \"glyph_idx\", \"x\", \"prev_kern\"])\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36mmake_dataclass\u001b[0;34m(cls_name, fields, bases, namespace, init, repr, eq, order, unsafe_hash, frozen)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(_cls, init, repr, eq, order, unsafe_hash, frozen)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type)\u001b[0m\n",
      "\u001b[0;32m~/envs/icra2020/lib/python3.7/site-packages/dataclasses.py\u001b[0m in \u001b[0;36m_is_classvar\u001b[0;34m(a_type, typing)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'typing' has no attribute '_ClassVar'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('icra2020': virtualenv)"
  },
  "interpreter": {
   "hash": "60804e52b8737fe4a2924f389a8fe9de2e4f169d8622d7a4235b16a8302d0ad7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}